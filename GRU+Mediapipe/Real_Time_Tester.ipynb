{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85dba3de-322a-48ae-a209-1cdd93b0e842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the code here to test the model in real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a0e29c4-c9ef-405a-8c70-b4452d918552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04ac37a0-c792-4149-be75-11c8e7dc2377",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing=mp.solutions.drawing_utils\n",
    "mp_holistic=mp.solutions.holistic\n",
    "holistic=mp_holistic.Holistic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7397a825-69d2-40db-b270-e16f5def315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "style1=mp_drawing.DrawingSpec((71, 237, 212),1,1)\n",
    "style2=mp_drawing.DrawingSpec((67, 73, 247),2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08196a50-6822-40bf-9faf-2b6b59497710",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def draw_landmarks(img,results):\n",
    "    mp_drawing.draw_landmarks(img,results.left_hand_landmarks,mp_holistic.HAND_CONNECTIONS,style1,style2)\n",
    "    mp_drawing.draw_landmarks(img,results.right_hand_landmarks,mp_holistic.HAND_CONNECTIONS,style1,style2)\n",
    "    mp_drawing.draw_landmarks(img,results.pose_landmarks,mp_holistic.POSE_CONNECTIONS,style1,style2)\n",
    "    mp_drawing.draw_landmarks(img,results.face_landmarks,mp_holistic.FACEMESH_CONTOURS,style1,style2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60c3770f-6b0c-43c4-aedc-74d755e91d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_landmarks(results):\n",
    "    face=np.array([[landmark.x,landmark.y,landmark.z] for landmark in results.face_landmarks.landmark]).flatten()  if results.face_landmarks else np.zeros(1404)\n",
    "    pose=np.array([[landmark.x,landmark.y,landmark.z , landmark.visibility] for landmark in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)\n",
    "    rh=np.array([[landmark.x,landmark.y,landmark.z] for landmark in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(63)\n",
    "    lh=np.array([[landmark.x,landmark.y,landmark.z] for landmark in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(63)\n",
    "\n",
    "    return np.concatenate([pose,face,lh,rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaa11fa1-6e96-4ee7-aa56-a965cfaa69f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap=cv.VideoCapture(0)\n",
    "white=cv.resize(cv.imread('white.png'),(900,500))\n",
    "while True:\n",
    "    _,frame=cap.read()\n",
    "    # mediapipe operations\n",
    "    img=cv.cvtColor(frame,cv.COLOR_BGR2RGB)\n",
    "    results=holistic.process(img)\n",
    "    img=cv.cvtColor(img,cv.COLOR_RGB2BGR)\n",
    "    draw_landmarks(img,results)\n",
    "    landmarks=extract_landmarks(results)\n",
    "    # print(landmarks)\n",
    "    \n",
    "    cv.imshow('frame',img)\n",
    "    cv.imshow('white',white)\n",
    "    if cv.waitKey(1)==27:\n",
    "        cap.release()\n",
    "        break\n",
    "\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d513c581-9f9c-4028-bb92-45947d155c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53e091e6-0398-4f1c-8798-638f8a096663",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model('gru_isl_model_extra_v2.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e33bda2a-c0d2-4088-a425-53389b8097f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bed01c3-07de-4600-8149-c45a8ff05fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions=pickle.load(open('v2/v2_classes.pickle','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dabd0644-4091-48a0-a5d6-f85dd1dd7f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blank', 'hello', 'how are you', 'sorry', 'thank you', 'welcome']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03493677-6a18-46f2-828f-89bfd3629208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 900, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "white.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cee1143-3c68-4a4b-8d92-004a4aaab07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    }
   ],
   "source": [
    "sentence=[]\n",
    "frames=[]\n",
    "\n",
    "threshold=0.7  # change acc to detections\n",
    "\n",
    "white=cv.imread(\"white.png\")\n",
    "\n",
    "cap=cv.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    _,frame=cap.read()\n",
    "\n",
    "    img=cv.cvtColor(frame,cv.COLOR_BGR2RGB)\n",
    "    results=holistic.process(img)\n",
    "    img=cv.cvtColor(img,cv.COLOR_RGB2BGR)\n",
    "    draw_landmarks(img,results)\n",
    "    landmarks=extract_landmarks(results)\n",
    "    # draw_landmarks(image,results)\n",
    "    # # landmarks=extract_landmarks(results)\n",
    "    # frames.insert(0,extract_landmarks(results))\n",
    "    frames.append(landmarks)\n",
    "    \n",
    "    if len(frames)==30:\n",
    "        x=np.array(frames)\n",
    "        frames.clear()\n",
    "        # res_1=actions[np.argmax(model.predict(np.expand_dims(x,0))[0])]\n",
    "        res_1=model.predict(np.expand_dims(x,0))[0]  # this has the array of predictions\n",
    "        # sentence.append(res)\n",
    "    \n",
    "    # arrangement logic of words \n",
    "        if np.max(res_1)>=threshold:  # if our predicted value is greater than the threshold\n",
    "            res_2=actions[np.argmax(res_1)]  #  predicted output will be stored in res_2 eg=> thanks\n",
    "            if len(sentence)>0:  # if length of sentence array is greater than 0 which means there are some words present.\n",
    "                if res_2!=sentence[-1]:  # if the latest outcome is not equal to the latest predicted outcome\n",
    "                    sentence.append(res_2)\n",
    "                \n",
    "            else: # if the sentence lenght is 0 which means its the first prediction\n",
    "                sentence.append(res_2) \n",
    "            \n",
    "     \n",
    "    cv.putText(white,' '.join(sentence),(10,20),cv.FONT_HERSHEY_COMPLEX_SMALL,1,(0,0,0),1,cv.LINE_AA)\n",
    "    if cv.waitKey(1)==27:\n",
    "        cap.release()\n",
    "        cv.destroyAllWindows()\n",
    "        break\n",
    "\n",
    "    cv.imshow(\"frame\",img)\n",
    "    cv.imshow('ur_sentence',white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d2e97d4-36bc-4288-8002-e0cbc28b687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537bf52a-fc0f-48f9-b440-212322967f72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
